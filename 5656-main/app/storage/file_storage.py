"""
BATCH 48: File-based storage for user balances (NO DATABASE)

Ð¥Ñ€Ð°Ð½Ð¸Ñ‚ Ð±Ð°Ð»Ð°Ð½ÑÑ‹ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹ Ð² JSON Ñ„Ð°Ð¹Ð»Ðµ + auto-commit Ð² GitHub.
ÐŸÐ¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ persistent, Ð¿ÐµÑ€ÐµÐ¶Ð¸Ð²Ð°ÐµÑ‚ Ð´ÐµÐ¿Ð»Ð¾Ð¸.
"""

import json
import logging
import os
import asyncio
import shutil
import subprocess
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


class FileStorage:
    """
    File-based storage Ð´Ð»Ñ Ð±Ð°Ð»Ð°Ð½ÑÐ¾Ð² Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹.
    
    Features:
    - JSON Ñ„Ð°Ð¹Ð» Ð´Ð»Ñ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð±Ð°Ð»Ð°Ð½ÑÐ¾Ð² (Ð¸Ð·Ð¾Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð¿Ð¾ BOT_TOKEN)
    - Auto-commit Ð² GitHub Ð¿Ð¾ÑÐ»Ðµ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹
    - Pull Ð½Ð° ÑÑ‚Ð°Ñ€Ñ‚Ðµ (Ð°ÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ)
    - Thread-safe Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸
    - ÐŸÐ¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð±ÐµÐ· PostgreSQL
    - Multi-bot support (ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð±Ð¾Ñ‚ = ÑÐ²Ð¾Ð¹ Ñ„Ð°Ð¹Ð»)
    """
    
    def __init__(self, data_file: str = "data/user_balances.json"):
        # BATCH 48: Multi-bot isolation by BOT_TOKEN
        self.data_file = self._get_isolated_data_file(data_file)
        self.data_dir = self.data_file.parent
        self.lock = asyncio.Lock()
        
        # BATCH 48.12: In-memory cache (CRITICAL performance improvement!)
        # Prevents blocking file I/O on EVERY get_balance() call
        self._cache: Optional[Dict[str, Any]] = None
        self._cache_valid = False
        
        # BATCH 48.37: In-memory job storage for callback reconciliation
        # Jobs stored in memory with TTL (1 hour) for callback matching
        self._jobs: Dict[str, Dict[str, Any]] = {}  # task_id -> job_info
        self._jobs_created_at: Dict[str, datetime] = {}  # task_id -> created_at
        self._jobs_ttl_seconds = 3600  # 1 hour TTL
        
        # BATCH 48.42: Referral system (in-memory for FileStorage)
        self._referrals: Dict[int, int] = {}  # user_id -> referrer_id
        self._referrals_reverse: Dict[int, List[int]] = {}  # referrer_id -> [user_id, ...]
        # BATCH 48.44: Free usage tracking in NO DATABASE MODE
        self._referral_bonuses: Dict[int, int] = {}  # user_id -> bonus_generations
        self._free_usage: List[Dict[str, Any]] = []  # List of {user_id, model_id, job_id, created_at}
        self._free_usage_max_size: int = 10000  # Prevent memory leak - limit to 10K entries
        self._free_usage_ttl_seconds: int = 86400  # 24 hours TTL for free usage records
        
        # Ensure data directory exists
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize file if not exists
        if not self.data_file.exists():
            self._init_file()
        
        logger.info(f"âœ… FileStorage initialized: {self.data_file}")
    
    def _get_isolated_data_file(self, default_file: str) -> Path:
        """
        Get isolated data file path based on BOT_TOKEN.
        
        Each bot gets its own file:
        - data/user_balances_bot_123456789.json
        - data/user_balances_bot_987654321.json
        
        This prevents balance mixing when multiple people use same GitHub repo.
        """
        bot_token = os.getenv("BOT_TOKEN", "")
        
        if not bot_token:
            logger.debug("BOT_TOKEN not found, using default file (expected in single-bot setups)")
            return Path(default_file)
        
        # Extract bot_id from token (first part before ":")
        # Format: 123456789:ABCdefGHIjklMNOpqrSTUvwxYZ
        try:
            bot_id = bot_token.split(":")[0]
            
            # Create isolated filename
            base_dir = Path(default_file).parent
            isolated_file = base_dir / f"user_balances_bot_{bot_id}.json"
            
            logger.info(f"ðŸ”’ Multi-bot isolation: bot_id={bot_id}, file={isolated_file.name}")
            return isolated_file
        
        except Exception as e:
            logger.error(f"âŒ Failed to extract bot_id from BOT_TOKEN: {e}")
            return Path(default_file)
    
    def _init_file(self):
        """
        Initialize empty JSON file.
        
        NOTE: This is called from __init__ (sync context), but file is small
        so blocking is acceptable (~1ms). For async init, see init_file_storage().
        """
        bot_token = os.getenv("BOT_TOKEN", "")
        bot_id = bot_token.split(":")[0] if bot_token else "unknown"
        
        initial_data = {
            "users": {},
            "metadata": {
                "bot_id": bot_id,
                "bot_file": self.data_file.name,
                "created_at": datetime.now().isoformat(),
                "version": "1.0",
                "description": f"User balances for bot {bot_id} - NO DATABASE MODE"
            },
            "referrals": {},  # BATCH 48.44: Referral system
            "referral_bonuses": {},  # BATCH 48.44: Referral bonuses
            "free_usage": []  # BATCH 48.44: Free usage tracking
        }
        # Small file, blocking OK in __init__ (~1ms)
        with open(self.data_file, 'w', encoding='utf-8') as f:
            json.dump(initial_data, f, indent=2, ensure_ascii=False)
        logger.info(f"âœ… Initialized {self.data_file} for bot_id={bot_id}")
    
    async def _load_data(self, force_reload: bool = False) -> Dict[str, Any]:
        """
        Load data from JSON file with caching and error handling.
        
        Args:
            force_reload: If True, bypass cache and reload from file
            
        Returns:
            Data dictionary with users, metadata, etc.
        """
        """
        Load data from JSON file with in-memory caching.
        
        BATCH 48.12: CRITICAL performance improvement!
        - Uses in-memory cache to avoid file I/O on EVERY get_balance()
        - Async file I/O (non-blocking)
        - force_reload=True to invalidate cache
        
        Performance: 5-10ms â†’ 0.001ms per get_balance()!
        """
        # Return cached data if valid
        if self._cache_valid and not force_reload and self._cache is not None:
            return self._cache
        
        # Load from file (async, non-blocking)
        try:
            def _read_json():
                # BATCH 48.31: Handle UTF-8 BOM (some editors add BOM)
                with open(self.data_file, 'r', encoding='utf-8-sig') as f:
                    return json.load(f)
            
            # BATCH 48.15: Add timeout for file operations (prevent hanging)
            data = await asyncio.wait_for(
                asyncio.to_thread(_read_json),
                timeout=10.0
            )
            
            # BATCH 48.44: Load referrals and free_usage from JSON into memory
            # Ensure backward compatibility
            data.setdefault("referrals", {})
            data.setdefault("referral_bonuses", {})
            data.setdefault("free_usage", [])
            
            # Load referrals into memory
            self._referrals = {}
            self._referrals_reverse = {}
            for user_str, referrer_str in data.get("referrals", {}).items():
                try:
                    user_id = int(user_str)
                    referrer_id = int(referrer_str)
                    self._referrals[user_id] = referrer_id
                    if referrer_id not in self._referrals_reverse:
                        self._referrals_reverse[referrer_id] = []
                    if user_id not in self._referrals_reverse[referrer_id]:
                        self._referrals_reverse[referrer_id].append(user_id)
                except (ValueError, TypeError):
                    logger.warning(f"[FileStorage] Invalid referral entry: {user_str} -> {referrer_str}")
            
            # Load referral bonuses into memory
            self._referral_bonuses = {}
            for user_str, bonus in data.get("referral_bonuses", {}).items():
                try:
                    user_id = int(user_str)
                    self._referral_bonuses[user_id] = int(bonus)
                except (ValueError, TypeError):
                    logger.warning(f"[FileStorage] Invalid referral bonus entry: {user_str} -> {bonus}")
            
            # Load free usage into memory
            self._free_usage = data.get("free_usage", [])
            if not isinstance(self._free_usage, list):
                logger.warning(f"[FileStorage] Invalid free_usage format, resetting to empty list")
                self._free_usage = []
            
            # Update cache
            self._cache = data
            self._cache_valid = True
            
            return data
        
        except asyncio.TimeoutError:
            logger.error(f"âŒ CRITICAL: File load timeout after 10s for {self.data_file}")
            # Return cached data if available, otherwise empty
            if self._cache is not None:
                logger.warning("âš ï¸ Using stale cache due to timeout")
                return self._cache
            empty_data = {"users": {}, "metadata": {}}
            self._cache = empty_data
            self._cache_valid = False
            return empty_data
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON decode error in {self.data_file}: {e}")
            # Try to restore from backup
            backup_file = self.data_file.with_suffix('.json.backup')
            if backup_file.exists():
                logger.warning(f"ðŸ”„ Restoring from backup: {backup_file}")
                try:
                    import shutil
                    await asyncio.to_thread(shutil.copy2, backup_file, self.data_file)
                    # Retry loading after restore
                    return await self._load_data(force_reload=True)
                except Exception as restore_error:
                    logger.error(f"âŒ Failed to restore from backup: {restore_error}")
            
            # Return empty structure if file is corrupted and restore failed
            empty_data = {"users": {}, "metadata": {}}
            self._cache = empty_data
            self._cache_valid = False  # Don't cache empty data
            return empty_data
        except Exception as e:
            logger.error(f"âŒ Failed to load {self.data_file}: {e}", exc_info=True)
            # Return empty structure if file is corrupted
            empty_data = {"users": {}, "metadata": {}}
            self._cache = empty_data
            self._cache_valid = False  # Don't cache empty data
            return empty_data
    
    async def _save_data(self, data: Dict[str, Any]):
        """
        Save data to JSON file with backup and validation.
        
        BATCH 48.8: BULLETPROOF save with backup + validation.
        BATCH 48.11: Async file I/O (non-blocking).
        BATCH 48.12: Invalidate cache after save.
        BATCH 48.15: Add timeout for file operations.
        
        Args:
            data: Data dictionary to save
            
        Raises:
            ValueError: If data structure is invalid
            asyncio.TimeoutError: If save operation times out
        """
        # CRITICAL: Validate input
        if not isinstance(data, dict):
            logger.error(f"[FileStorage] Invalid data type in _save_data: {type(data)}")
            raise ValueError(f"data must be a dict, got {type(data)}")
        
        # BATCH 48.15: Add timeout for file operations (prevent hanging)
        try:
            await asyncio.wait_for(self._save_data_internal(data), timeout=30.0)
        except asyncio.TimeoutError:
            logger.error(f"âŒ CRITICAL: File save timeout after 30s for {self.data_file}")
            raise
        except Exception as e:
            logger.error(f"[FileStorage] Error in _save_data: {e}", exc_info=True)
            raise
    
    async def _save_data_internal(self, data: Dict[str, Any]):
        """Internal save implementation."""
        try:
            # Update metadata
            data["metadata"]["updated_at"] = datetime.now().isoformat()
            
            # STEP 1: Create backup of current file (non-blocking)
            if self.data_file.exists():
                backup_file = self.data_file.with_suffix('.json.backup')
                await asyncio.to_thread(shutil.copy2, self.data_file, backup_file)
                logger.debug(f"ðŸ“‹ Backup created: {backup_file.name}")
            
            # STEP 2: Validate JSON structure
            if "users" not in data or "metadata" not in data:
                raise ValueError("Invalid JSON structure: missing 'users' or 'metadata'")
            
            # BATCH 48.44: Sync referrals and free_usage from memory to JSON
            # Ensure fields exist
            data.setdefault("referrals", {})
            data.setdefault("referral_bonuses", {})
            data.setdefault("free_usage", [])
            
            # Save referrals from memory to JSON
            data["referrals"] = {str(user_id): str(referrer_id) for user_id, referrer_id in self._referrals.items()}
            
            # Save referral bonuses from memory to JSON
            data["referral_bonuses"] = {str(user_id): bonus for user_id, bonus in self._referral_bonuses.items()}
            
            # Save free usage from memory to JSON
            data["free_usage"] = self._free_usage
            
            # STEP 3: Write to file (non-blocking)
            def _write_json():
                with open(self.data_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)
            
            await asyncio.to_thread(_write_json)
            
            # STEP 4: Verify saved file is valid JSON (non-blocking)
            def _verify_json():
                with open(self.data_file, 'r', encoding='utf-8') as f:
                    json.load(f)  # Will raise if invalid JSON
            
            await asyncio.to_thread(_verify_json)
            
            # BATCH 48.12: Update cache with new data
            self._cache = data.copy()
            self._cache_valid = True
            
            logger.debug(f"âœ… Saved and verified: {self.data_file.name}")
        
        except Exception as e:
            logger.error(f"âŒ CRITICAL: Failed to save data: {e}")
            # Invalidate cache on error
            self._cache_valid = False
            
            # Try to restore from backup (non-blocking)
            backup_file = self.data_file.with_suffix('.json.backup')
            if backup_file.exists():
                logger.warning("ðŸ”„ Restoring from backup...")
                await asyncio.to_thread(shutil.copy2, backup_file, self.data_file)
                logger.info("âœ… Restored from backup")
            raise
    
    async def get_balance(self, user_id: int) -> float:
        """
        Get user balance.
        
        Args:
            user_id: Telegram user ID
            
        Returns:
            User balance (default: 0.0)
        """
        # CRITICAL: Validate input
        if not isinstance(user_id, int) or user_id <= 0:
            logger.warning(f"[FileStorage] Invalid user_id: {user_id} (must be positive integer)")
            return 0.0
        
        try:
            async with self.lock:
                data = await self._load_data()  # BATCH 48.12: await async + cache
                user_key = str(user_id)
                
                if user_key in data["users"]:
                    balance = data["users"][user_key].get("balance", 0.0)
                    # CRITICAL: Validate balance is a number
                    if not isinstance(balance, (int, float)):
                        logger.error(f"[FileStorage] Invalid balance type for user {user_id}: {type(balance)}")
                        return 0.0
                    return float(balance)
                else:
                    # User not found, return 0 (expected for new users)
                    logger.debug(f"[FileStorage] User {user_id} not found, returning default balance 0.0")
                    return 0.0
        except Exception as e:
            logger.error(f"[FileStorage] Error getting balance for user {user_id}: {e}", exc_info=True)
            return 0.0  # Fail-safe: return 0 on error
    
    async def set_balance(self, user_id: int, amount: float, auto_commit: bool = True):
        """
        Set user balance.
        
        Args:
            user_id: Telegram user ID
            amount: New balance
            auto_commit: Auto-commit to GitHub
        """
        # CRITICAL: Validate input
        if not isinstance(user_id, int) or user_id <= 0:
            raise ValueError(f"Invalid user_id: {user_id} (must be positive integer)")
        
        if not isinstance(amount, (int, float)):
            raise ValueError(f"Invalid amount type: {type(amount)} (must be number)")
        
        if amount < 0:
            logger.error(f"[FileStorage] âŒ CRITICAL: Attempt to set negative balance: user={user_id}, amount={amount}")
            raise ValueError(f"Invalid amount: {amount} (must be non-negative number)")
        
        # CRITICAL: Check for NaN or Infinity
        import math
        if math.isnan(amount) or math.isinf(amount):
            logger.error(f"[FileStorage] âŒ CRITICAL: Invalid amount (NaN/Inf): user={user_id}, amount={amount}")
            raise ValueError(f"Invalid amount: {amount} (NaN or Infinity)")
        
        async with self.lock:
            data = await self._load_data()  # BATCH 48.12: await async + cache
            user_key = str(user_id)
            
            # Create user entry if not exists
            if user_key not in data["users"]:
                data["users"][user_key] = {
                    "balance": 0.0,
                    "created_at": datetime.now().isoformat()
                }
            
            # Update balance
            old_balance = data["users"][user_key].get("balance", 0.0)
            
            # CRITICAL FIX: Final check before setting balance
            if amount < 0:
                logger.error(
                    f"âŒ CRITICAL: Balance would become negative: user={user_id} "
                    f"old={old_balance} new={amount}"
                )
                raise ValueError(f"Cannot set negative balance: {amount}")
            
            data["users"][user_key]["balance"] = amount
            data["users"][user_key]["updated_at"] = datetime.now().isoformat()
            
            # Save to file (BATCH 48.11: await async)
            await self._save_data(data)
            
            logger.info(f"ðŸ’° Balance updated: user={user_id}, {old_balance} â†’ {amount}")
            
            # Auto-commit to GitHub with GUARANTEED push
            if auto_commit:
                await self._commit_to_github(
                    commit_message=f"Balance update: user {user_id}, {old_balance:.2f} â†’ {amount:.2f}",
                    user_id=user_id,
                    old_balance=old_balance,
                    new_balance=amount
                )
    
    async def add_balance(self, user_id: int, amount: float, auto_commit: bool = True):
        """
        Add to user balance.
        
        Args:
            user_id: Telegram user ID
            amount: Amount to add (can be negative for subtraction)
            auto_commit: Auto-commit to GitHub
        """
        # CRITICAL: Validate inputs
        if not isinstance(user_id, int) or user_id <= 0:
            raise ValueError(f"Invalid user_id: {user_id} (must be positive integer)")
        if not isinstance(amount, (int, float)):
            raise ValueError(f"Invalid amount: {amount} (must be a number)")
        
        try:
            current_balance = await self.get_balance(user_id)
            new_balance = current_balance + amount
            
            # CRITICAL: Prevent negative balance
            if new_balance < 0:
                logger.error(
                    f"[FileStorage] âŒ CRITICAL: Balance would become negative: "
                    f"user={user_id} current={current_balance} amount={amount} new={new_balance}"
                )
                raise ValueError(f"Cannot add {amount} to balance {current_balance}: would result in negative balance")
            
            await self.set_balance(user_id, new_balance, auto_commit)
        except (ValueError, TypeError) as e:
            # Specific errors (validation, type errors)
            logger.error(f"[FileStorage] Error adding balance for user {user_id} (validation error): {e}", exc_info=True)
            raise
        except Exception as e:
            # Unexpected errors
            logger.error(f"[FileStorage] Error adding balance for user {user_id} (unexpected error): {e}", exc_info=True)
            raise
            raise
    
    async def subtract_balance(self, user_id: int, amount: float, auto_commit: bool = True) -> bool:
        """
        Subtract from user balance.
        
        Args:
            user_id: Telegram user ID
            amount: Amount to subtract
            auto_commit: Auto-commit to GitHub
            
        Returns:
            True if successful, False if insufficient balance
        """
        # BATCH 48.15: Validate input
        if not isinstance(user_id, int) or user_id <= 0:
            raise ValueError(f"Invalid user_id: {user_id} (must be positive integer)")
        if not isinstance(amount, (int, float)) or amount < 0:
            raise ValueError(f"Invalid amount: {amount} (must be non-negative number)")
        
        current_balance = await self.get_balance(user_id)
        
        if current_balance < amount:
            logger.warning(f"âš ï¸ Insufficient balance: user={user_id}, balance={current_balance}, required={amount}")
            return False
        
        new_balance = current_balance - amount
        
        # CRITICAL FIX: Defense-in-depth - verify balance won't become negative
        if new_balance < 0:
            logger.error(
                f"âŒ CRITICAL: Balance would become negative: user={user_id} "
                f"current={current_balance} amount={amount} new={new_balance}"
            )
            return False
        
        await self.set_balance(user_id, new_balance, auto_commit)
        return True
    
    async def get_all_users(self) -> Dict[str, Dict[str, Any]]:
        """
        Get all users with balances.
        
        Returns:
            Dictionary of user_id -> user_data
        """
        try:
            async with self.lock:
                data = await self._load_data()  # BATCH 48.12: await async + cache
                users = data.get("users", {})
                
                # CRITICAL: Validate users structure
                if not isinstance(users, dict):
                    logger.error(f"[FileStorage] Invalid users structure: {type(users)}")
                    return {}
                
                return users
        except (KeyError, TypeError, json.JSONDecodeError) as e:
            # Specific errors (data corruption, JSON errors)
            logger.error(f"[FileStorage] Error getting all users (data error): {e}", exc_info=True)
            return {}  # Fail-safe: return empty dict on error
        except Exception as e:
            # Unexpected errors
            logger.error(f"[FileStorage] Error getting all users (unexpected error): {e}", exc_info=True)
            return {}  # Fail-safe: return empty dict on error
    
    async def add_generation_job(
        self,
        user_id: int,
        model_id: str,
        model_name: str,
        params: Dict[str, Any],
        price: float,
        task_id: Optional[str] = None,
        status: str = "queued"
    ) -> str:
        """
        Add generation job to in-memory storage (for callback reconciliation).
        BATCH 48.37: Jobs stored in memory with TTL for callback matching.
        
        Returns:
            task_id: The task ID (generated if not provided)
        """
        # CRITICAL: Validate inputs
        if not isinstance(user_id, int) or user_id <= 0:
            raise ValueError(f"Invalid user_id: {user_id} (must be positive integer)")
        if not model_id or not isinstance(model_id, str):
            raise ValueError(f"Invalid model_id: {model_id} (must be non-empty string)")
        if not model_name or not isinstance(model_name, str):
            raise ValueError(f"Invalid model_name: {model_name} (must be non-empty string)")
        if not isinstance(params, dict):
            raise ValueError(f"Invalid params: {params} (must be dict)")
        if not isinstance(price, (int, float)) or price < 0:
            raise ValueError(f"Invalid price: {price} (must be non-negative number)")
        if status and not isinstance(status, str):
            raise ValueError(f"Invalid status: {status} (must be string)")
        
        import uuid
        job_id = task_id or str(uuid.uuid4())
        
        try:
            async with self.lock:  # CRITICAL: Thread-safe access
                # BATCH 48.37: Store job in memory for callback reconciliation
                # BATCH 48.38: Extract chat_id from params if available
                chat_id = params.get('chat_id') if isinstance(params, dict) else None
                
                # CRITICAL: Validate chat_id if present
                if chat_id is not None:
                    if not isinstance(chat_id, int) or chat_id <= 0:
                        logger.warning(f"[FileStorage] Invalid chat_id in params: {chat_id}")
                        chat_id = None
                
                self._jobs[job_id] = {
                    'job_id': job_id,
                    'user_id': user_id,
                    'chat_id': chat_id,  # BATCH 48.38: Store chat_id for delivery
                    'model_id': model_id,
                    'model_name': model_name,
                    'params': params,
                    'price': price,
                    'price_rub': price,  # For compatibility with mark_delivered
                    'status': status,
                    'created_at': datetime.now().isoformat(),
                    'task_id': job_id  # For compatibility with find_job_by_task_id
                }
                self._jobs_created_at[job_id] = datetime.now()
                
                # CRITICAL: Prevent memory leak - cleanup old jobs
                await self._cleanup_old_jobs()
                
                logger.debug(f"[FileStorage] Job added: job_id={job_id}, user_id={user_id}, model_id={model_id}")
        except (ValueError, TypeError, KeyError) as e:
            # Specific errors (validation, type errors, missing keys)
            logger.error(f"[FileStorage] Error adding generation job (validation error): {e}", exc_info=True)
            raise
        except Exception as e:
            # Unexpected errors
            logger.error(f"[FileStorage] Error adding generation job (unexpected error): {e}", exc_info=True)
            raise
        
        # Cleanup old jobs (older than TTL)
        await self._cleanup_old_jobs()
        
        logger.debug(f"[FileStorage] Job stored in memory: task_id={job_id}, user_id={user_id}, model_id={model_id}")
        return job_id
    
    async def _cleanup_old_jobs(self):
        """
        Remove jobs older than TTL from memory.
        
        CRITICAL: Thread-safe cleanup with error handling.
        """
        try:
            now = datetime.now()
            expired_task_ids = []
            
            # CRITICAL: Validate _jobs_created_at structure
            if not isinstance(self._jobs_created_at, dict):
                logger.error(f"[FileStorage] Invalid _jobs_created_at type: {type(self._jobs_created_at)}")
                return
            
            for task_id, created_at in self._jobs_created_at.items():
                try:
                    if not isinstance(created_at, datetime):
                        # Try to parse if it's a string
                        if isinstance(created_at, str):
                            created_at = datetime.fromisoformat(created_at)
                        else:
                            logger.warning(f"[FileStorage] Invalid created_at type for task_id={task_id}: {type(created_at)}")
                            continue
                    
                    if (now - created_at).total_seconds() > self._jobs_ttl_seconds:
                        expired_task_ids.append(task_id)
                except (ValueError, TypeError) as e:
                    logger.warning(f"[FileStorage] Error processing task_id={task_id} in cleanup: {e}")
                    # Remove invalid entries
                    expired_task_ids.append(task_id)
            
            for task_id in expired_task_ids:
                self._jobs.pop(task_id, None)
                self._jobs_created_at.pop(task_id, None)
            
            if expired_task_ids:
                logger.debug(f"[FileStorage] Cleaned up {len(expired_task_ids)} expired jobs from memory")
            
            # CRITICAL: Also limit size to prevent memory leak (defense-in-depth)
            max_jobs = 5000  # Limit to 5K jobs in memory
            if len(self._jobs) > max_jobs:
                # Remove oldest jobs (by created_at)
                try:
                    sorted_jobs = sorted(self._jobs_created_at.items(), key=lambda x: x[1] if isinstance(x[1], datetime) else datetime.min)
                    remove_count = len(self._jobs) - max_jobs
                    for task_id, _ in sorted_jobs[:remove_count]:
                        self._jobs.pop(task_id, None)
                        self._jobs_created_at.pop(task_id, None)
                    logger.debug(f"[FileStorage] Jobs exceeded max size, removed {remove_count} oldest jobs")
                except Exception as e:
                    logger.error(f"[FileStorage] Error sorting jobs for cleanup: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"[FileStorage] Error in _cleanup_old_jobs: {e}", exc_info=True)
    
    async def _cleanup_old_free_usage(self):
        """
        Remove free usage records older than TTL from memory.
        
        CRITICAL: Thread-safe cleanup with error handling.
        """
        try:
            now = datetime.now()
            cutoff_time = now - timedelta(seconds=self._free_usage_ttl_seconds)
            
            original_count = len(self._free_usage)
            valid_entries = []
            
            for entry in self._free_usage:
                if not isinstance(entry, dict):
                    logger.warning(f"[FileStorage] Invalid entry type in _free_usage: {type(entry)}")
                    continue
                
                try:
                    created_at_str = entry.get("created_at")
                    if not created_at_str:
                        logger.warning(f"[FileStorage] Missing created_at in free_usage entry: {entry}")
                        continue
                    
                    created_at = datetime.fromisoformat(created_at_str)
                    if created_at > cutoff_time:
                        valid_entries.append(entry)
                except (ValueError, TypeError) as e:
                    logger.warning(f"[FileStorage] Invalid created_at format in free_usage entry: {entry.get('created_at')}, error: {e}")
                    # Skip invalid entries
            
            self._free_usage = valid_entries
            removed_count = original_count - len(self._free_usage)
            
            if removed_count > 0:
                logger.debug(f"[FileStorage] Cleaned up {removed_count} expired free usage records")
        except Exception as e:
            logger.error(f"[FileStorage] Error in _cleanup_old_free_usage: {e}", exc_info=True)

    async def find_job_by_task_id(self, task_id: str) -> Optional[Dict[str, Any]]:
        """
        Find job by task_id from in-memory storage.
        BATCH 48.37: Jobs stored in memory for callback reconciliation.
        
        Args:
            task_id: Task ID to search for
            
        Returns:
            Job dict if found, None otherwise
        """
        # CRITICAL: Validate input
        if not task_id or not isinstance(task_id, str):
            logger.warning(f"[FileStorage] Invalid task_id in find_job_by_task_id: {task_id}")
            return None
        
        try:
            # Cleanup old jobs before search
            await self._cleanup_old_jobs()
            
            # BATCH 48.37: Return job from memory if exists
            async with self.lock:  # CRITICAL: Thread-safe access
                job = self._jobs.get(task_id)
                if job:
                    if not isinstance(job, dict):
                        logger.error(f"[FileStorage] Invalid job type in _jobs: {type(job)}")
                        return None
                    logger.debug(f"[FileStorage] Job found in memory: task_id={task_id}")
                    return job.copy()  # Return copy to prevent mutations
            
            logger.debug(f"[FileStorage] Job not found in memory: task_id={task_id}")
            return None
        except (KeyError, TypeError, AttributeError) as e:
            # Specific errors (missing keys, type errors, attribute errors)
            logger.error(f"[FileStorage] Error finding job by task_id {task_id} (data error): {e}", exc_info=True)
            return None  # Fail-safe: return None on error
        except Exception as e:
            # Unexpected errors
            logger.error(f"[FileStorage] Error finding job by task_id {task_id} (unexpected error): {e}", exc_info=True)
            return None  # Fail-safe: return None on error

    async def update_job_status(
        self,
        job_id: str,
        status: str,
        result_urls: Optional[List[str]] = None,
        error_message: Optional[str] = None,
        delivered: bool = False
    ) -> None:
        """
        Update job status in memory storage.
        BATCH 48.38: Update job status for callback reconciliation.
        
        CRITICAL: Thread-safe update with lock to prevent race conditions.
        """
        # CRITICAL: Validate inputs
        if not job_id or not isinstance(job_id, str):
            logger.warning(f"[FileStorage] Invalid job_id in update_job_status: {job_id}")
            return
        if not status or not isinstance(status, str):
            logger.warning(f"[FileStorage] Invalid status in update_job_status: {status}")
            return
        
        async with self.lock:  # CRITICAL: Prevent race conditions
            # BATCH 48.38: Update job in memory
            if job_id in self._jobs:
                old_status = self._jobs[job_id].get('status', 'unknown')
                self._jobs[job_id]['status'] = status
                if result_urls is not None:
                    # CRITICAL: Validate result_urls is a list of strings
                    if not isinstance(result_urls, list):
                        logger.warning(f"[FileStorage] Invalid result_urls type: {type(result_urls)}")
                    else:
                        # Validate all items are strings
                        valid_urls = []
                        for url in result_urls:
                            if isinstance(url, str) and url.strip():
                                valid_urls.append(url.strip())
                            else:
                                logger.warning(f"[FileStorage] Invalid URL in result_urls: {url} (type: {type(url)})")
                        
                        if valid_urls:
                            self._jobs[job_id]['result_urls'] = valid_urls
                        else:
                            logger.warning(f"[FileStorage] No valid URLs in result_urls for job_id={job_id}")
                if error_message is not None:
                    self._jobs[job_id]['error_message'] = str(error_message)
                if delivered:
                    self._jobs[job_id]['delivered_at'] = datetime.now().isoformat()
                self._jobs[job_id]['updated_at'] = datetime.now().isoformat()
                logger.debug(f"[FileStorage] Job status updated: job_id={job_id}, {old_status} â†’ {status}")
            else:
                logger.debug(f"[FileStorage] Job not found for update: job_id={job_id}")

    async def try_acquire_delivery_lock(self, task_id: str, timeout_minutes: int = 5) -> Optional[Dict[str, Any]]:
        """
        Try to acquire delivery lock for a task (in-memory implementation).
        BATCH 48.38: Prevents duplicate deliveries in FileStorage.
        
        CRITICAL: Thread-safe lock acquisition to prevent race conditions.
        
        Returns:
            Job dict if lock acquired, None if already delivered or locked
        """
        # CRITICAL: Validate inputs
        if not task_id or not isinstance(task_id, str):
            logger.warning(f"[FileStorage] Invalid task_id in try_acquire_delivery_lock: {task_id}")
            return None
        
        if not isinstance(timeout_minutes, int) or timeout_minutes <= 0:
            logger.warning(f"[FileStorage] Invalid timeout_minutes in try_acquire_delivery_lock: {timeout_minutes}")
            timeout_minutes = 5  # Use default
        
        async with self.lock:  # CRITICAL: Thread-safe access
            # BATCH 48.38: Simple in-memory lock using delivered_at and delivering_at
            job = self._jobs.get(task_id)
            if not job:
                logger.debug(f"[FileStorage] Job not found for delivery lock: task_id={task_id}")
                return None
            
            # Check if already delivered
            if job.get('delivered_at'):
                logger.debug(f"[FileStorage] Job already delivered: task_id={task_id}")
                return None
            
            # Check if currently delivering (with timeout)
            delivering_at_str = job.get('delivering_at')
            if delivering_at_str:
                try:
                    delivering_at = datetime.fromisoformat(delivering_at_str)
                    timeout_delta = timedelta(minutes=timeout_minutes)
                    if datetime.now() - delivering_at < timeout_delta:
                        logger.debug(f"[FileStorage] Job still locked: task_id={task_id}, delivering_at={delivering_at_str}")
                        return None  # Still locked
                    else:
                        logger.debug(f"[FileStorage] Lock expired for task_id={task_id}, acquiring new lock")
                except (ValueError, TypeError) as e:
                    logger.warning(f"[FileStorage] Invalid delivering_at format for task_id={task_id}: {e}")
                    # Invalid date, treat as stale - continue to acquire lock
            
            # Acquire lock
            job['delivering_at'] = datetime.now().isoformat()
            logger.debug(f"[FileStorage] Delivery lock acquired: task_id={task_id}")
            return job.copy()

    async def mark_delivered(self, task_id: str, success: bool = True, error: Optional[str] = None) -> None:
        """
        Mark job as delivered (or failed delivery).
        BATCH 48.38: Mark delivery status in memory.
        
        CRITICAL: Charge balance ONLY after successful delivery.
        This ensures user only pays when they actually receive the result.
        
        CRITICAL: Thread-safe operation with lock to prevent race conditions.
        """
        # CRITICAL: Validate inputs
        if not task_id or not isinstance(task_id, str):
            logger.warning(f"[FileStorage] Invalid task_id in mark_delivered: {task_id}")
            return
        
        if not isinstance(success, bool):
            logger.warning(f"[FileStorage] Invalid success type in mark_delivered: {type(success)}")
            success = bool(success)
        
        async with self.lock:  # CRITICAL: Thread-safe access
            job = self._jobs.get(task_id)
        if job:
            if success:
                job['delivered_at'] = datetime.now().isoformat()
                job['delivering_at'] = None
                job['status'] = 'done'
                
                # CRITICAL: Charge balance ONLY after successful delivery
                user_id = job.get('user_id')
                price_rub = job.get('price_rub', 0.0)
                
                # CRITICAL: Validate user_id and price_rub
                if not isinstance(user_id, int) or user_id <= 0:
                    logger.error(f"[FileStorage] Invalid user_id in mark_delivered: {user_id} task_id={task_id}")
                    user_id = None
                
                if not isinstance(price_rub, (int, float)) or price_rub < 0:
                    logger.error(f"[FileStorage] Invalid price_rub in mark_delivered: {price_rub} task_id={task_id}")
                    price_rub = 0.0
                
                if user_id and price_rub > 0 and job.get('status') == 'done':
                    # Check if balance was already charged (idempotency)
                    if not job.get('balance_charged_after_delivery'):
                        try:
                            # Charge balance after successful delivery
                            success_charge = await self.subtract_balance(user_id, price_rub, auto_commit=True)
                            if success_charge:
                                job['balance_charged_after_delivery'] = True
                                logger.info(
                                    f"[FileStorage] [BALANCE_CHARGE_AFTER_DELIVERY] user={user_id} task_id={task_id} "
                                    f"price={price_rub} charged=True"
                                )
                            else:
                                logger.error(
                                    f"[FileStorage] [BALANCE_CHARGE_ERROR] user={user_id} task_id={task_id} "
                                    f"price={price_rub} failed to charge (insufficient balance or error)"
                                )
                        except Exception as e:
                            logger.error(
                                f"[FileStorage] [BALANCE_CHARGE_EXCEPTION] user={user_id} task_id={task_id} "
                                f"price={price_rub} error={e}", exc_info=True
                            )
                    else:
                        logger.debug(f"[FileStorage] Balance already charged for task_id={task_id} (idempotent)")
                elif user_id and price_rub == 0:
                    logger.debug(f"[FileStorage] Free model, no balance charge needed: task_id={task_id}")
                elif not user_id:
                    logger.warning(f"[FileStorage] Cannot charge balance: missing user_id for task_id={task_id}")
                
                logger.debug(f"[FileStorage] Job marked as delivered: task_id={task_id}")
            else:
                job['delivering_at'] = None
                if error:
                    job['error_message'] = (job.get('error_message') or '') + f"\n[DELIVERY_FAIL] {error}"
                logger.debug(f"[FileStorage] Job delivery failed: task_id={task_id}, error={error}")
        else:
            logger.debug(f"[FileStorage] Job not found for mark_delivered: task_id={task_id}")

    # ==================== REFERRALS ====================
    # BATCH 48.42: Referral system for free model limits
    
    async def set_referrer(self, user_id: int, referrer_id: int) -> None:
        """
        Set referrer for user (in-memory for FileStorage).
        BATCH 48.42: Referral system for free model limits.
        """
        # Validate input
        if not isinstance(user_id, int) or user_id <= 0:
            raise ValueError(f"Invalid user_id: {user_id}")
        if not isinstance(referrer_id, int) or referrer_id <= 0:
            raise ValueError(f"Invalid referrer_id: {referrer_id}")
        if user_id == referrer_id:
            raise ValueError("Cannot set self as referrer")
        
        async with self.lock:
            # Check if user already has referrer
            if user_id in self._referrals:
                logger.debug(f"[FileStorage] User {user_id} already has referrer {self._referrals[user_id]}")
                return
            
            # Set referrer in memory
            self._referrals[user_id] = referrer_id
            
            # Update reverse index
            if referrer_id not in self._referrals_reverse:
                self._referrals_reverse[referrer_id] = []
            if user_id not in self._referrals_reverse[referrer_id]:
                self._referrals_reverse[referrer_id].append(user_id)
            
            # BATCH 48.44: Save to JSON file
            data = await self._load_data()
            if "referrals" not in data:
                data["referrals"] = {}
            data["referrals"][str(user_id)] = str(referrer_id)
            await self._save_data(data)
            
            logger.info(f"[FileStorage] Set referrer: user={user_id} referrer={referrer_id}")
    
    async def get_referrer(self, user_id: int) -> Optional[int]:
        """
        Get referrer ID for user (in-memory for FileStorage).
        BATCH 48.42: Referral system for free model limits.
        
        Args:
            user_id: User ID
            
        Returns:
            Referrer ID or None if not found
        """
        # CRITICAL: Validate input
        if not isinstance(user_id, int) or user_id <= 0:
            logger.warning(f"[FileStorage] Invalid user_id in get_referrer: {user_id}")
            return None
        
        try:
            async with self.lock:
                return self._referrals.get(user_id)
        except (KeyError, AttributeError) as e:
            # Specific errors (missing keys, attribute errors)
            logger.error(f"[FileStorage] Error getting referrer for user {user_id} (data error): {e}", exc_info=True)
            return None
        except Exception as e:
            # Unexpected errors
            logger.error(f"[FileStorage] Error getting referrer for user {user_id} (unexpected error): {e}", exc_info=True)
            return None
    
    async def get_referrals(self, referrer_id: int) -> List[int]:
        """
        Get list of referrals for referrer (in-memory for FileStorage).
        BATCH 48.42: Referral system for free model limits.
        
        Args:
            referrer_id: Referrer user ID
            
        Returns:
            List of referred user IDs
        """
        # CRITICAL: Validate input
        if not isinstance(referrer_id, int) or referrer_id <= 0:
            logger.warning(f"[FileStorage] Invalid referrer_id in get_referrals: {referrer_id}")
            return []
        
        try:
            async with self.lock:
                referrals = self._referrals_reverse.get(referrer_id, []).copy()
                # CRITICAL: Validate return value is a list
                if not isinstance(referrals, list):
                    logger.error(f"[FileStorage] Invalid referrals structure for referrer {referrer_id}: {type(referrals)}")
                    return []
                return referrals
        except (KeyError, AttributeError, TypeError) as e:
            # Specific errors (missing keys, attribute errors, type errors)
            logger.error(f"[FileStorage] Error getting referrals for referrer {referrer_id} (data error): {e}", exc_info=True)
            return []  # Fail-safe: return empty list
        except Exception as e:
            # Unexpected errors
            logger.error(f"[FileStorage] Error getting referrals for referrer {referrer_id} (unexpected error): {e}", exc_info=True)
            return []  # Fail-safe: return empty list
    
    async def add_referral_bonus(self, referrer_id: int, bonus_generations: int = 5) -> None:
        """
        Add referral bonus (persisted to JSON in FileStorage).
        BATCH 48.44: Referral system for free model limits.
        """
        if not isinstance(referrer_id, int) or referrer_id <= 0:
            raise ValueError(f"Invalid referrer_id: {referrer_id}")
        if not isinstance(bonus_generations, int) or bonus_generations <= 0:
            raise ValueError(f"Invalid bonus_generations: {bonus_generations}")
        
        async with self.lock:
            # Update in memory
            current_bonus = self._referral_bonuses.get(referrer_id, 0)
            self._referral_bonuses[referrer_id] = current_bonus + bonus_generations
            
            # BATCH 48.44: Save to JSON file
            data = await self._load_data()
            if "referral_bonuses" not in data:
                data["referral_bonuses"] = {}
            data["referral_bonuses"][str(referrer_id)] = self._referral_bonuses[referrer_id]
            await self._save_data(data)
            
            logger.info(f"[FileStorage] Added {bonus_generations} bonus generations to referrer {referrer_id}. New total: {self._referral_bonuses[referrer_id]}")

    async def _save_orphan_callback(self, task_id: str, payload: Dict[str, Any]) -> None:
        """
        Save orphan callback (no-op in FileStorage - callbacks processed immediately via polling).
        BATCH 48.37: In FileStorage, callbacks are handled by polling, so orphan callbacks
        are expected and will be matched when job appears in memory.
        """
        # BATCH 48.37: In FileStorage, orphan callbacks are expected
        # They will be matched when job appears in memory (via polling)
        # Log at INFO level (not WARNING) as this is expected behavior
        logger.info(f"[FileStorage] Orphan callback received (will be matched by polling): task_id={task_id}")
        pass

    # ==================== FREE USAGE TRACKING ====================
    # BATCH 48.44: Free usage tracking in NO DATABASE MODE
    
    async def log_free_usage(self, user_id: int, model_id: str, job_id: Optional[str] = None) -> None:
        """
        Log free model usage in FileStorage.
        BATCH 48.44: Track free usage in NO DATABASE MODE.
        """
        if not isinstance(user_id, int) or user_id <= 0:
            raise ValueError(f"Invalid user_id: {user_id} (must be positive integer)")
        if not isinstance(model_id, str) or not model_id.strip():
            raise ValueError(f"Invalid model_id: {model_id} (must be non-empty string)")
        
        async with self.lock:
            # Check for duplicate (idempotency)
            if job_id:
                for entry in self._free_usage:
                    if entry.get("user_id") == user_id and \
                       entry.get("model_id") == model_id and \
                       entry.get("job_id") == job_id:
                        logger.debug(f"[FileStorage] Free usage already logged for job {job_id} (idempotent)")
                        return
            
            # Add to memory
            self._free_usage.append({
                "user_id": user_id,
                "model_id": model_id,
                "job_id": job_id,
                "created_at": datetime.now().isoformat()
            })
            
            # CRITICAL: Prevent memory leak - cleanup old entries and limit size
            await self._cleanup_old_free_usage()
            if len(self._free_usage) > self._free_usage_max_size:
                # Remove oldest 10% entries (LRU eviction)
                remove_count = self._free_usage_max_size // 10
                self._free_usage = self._free_usage[remove_count:]
                logger.debug(f"[FileStorage] Free usage list exceeded max size, removed {remove_count} oldest entries")
            
            # BATCH 48.44: Save to JSON file
            data = await self._load_data()
            data["free_usage"] = self._free_usage
            await self._save_data(data)
            
            logger.info(f"[FileStorage] Logged free usage: user={user_id}, model={model_id}, job={job_id}")
    
    async def get_daily_free_usage(self, user_id: int, model_id: str) -> int:
        """
        Get daily free usage count for a user and model in FileStorage.
        
        Args:
            user_id: User ID
            model_id: Model ID
            
        Returns:
            Count of free usages today
        """
        # CRITICAL: Validate inputs
        if not isinstance(user_id, int) or user_id <= 0:
            logger.warning(f"[FileStorage] Invalid user_id in get_daily_free_usage: {user_id}")
            return 0
        
        if not isinstance(model_id, str) or not model_id.strip():
            logger.warning(f"[FileStorage] Invalid model_id in get_daily_free_usage: {model_id}")
            return 0
        
        try:
            now = datetime.now()
            day_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
            
            async with self.lock:
                # Use in-memory data (already synced from JSON)
                count = 0
                for entry in self._free_usage:
                    if not isinstance(entry, dict):
                        logger.warning(f"[FileStorage] Invalid entry type in _free_usage: {type(entry)}")
                        continue
                    
                    if entry.get("user_id") == user_id and entry.get("model_id") == model_id:
                        try:
                            created_at_str = entry.get("created_at")
                            if not created_at_str:
                                logger.warning(f"[FileStorage] Missing created_at in free_usage entry: {entry}")
                                continue
                            
                            created_at = datetime.fromisoformat(created_at_str)
                            if created_at >= day_start:
                                count += 1
                        except (ValueError, KeyError, TypeError) as e:
                            logger.warning(f"[FileStorage] Invalid created_at format in free_usage: {entry.get('created_at')}, error: {e}")
                return count
        except (KeyError, TypeError, ValueError) as e:
            # Specific errors (missing keys, type errors, value errors)
            logger.error(f"[FileStorage] Error getting daily free usage for user {user_id}, model {model_id} (data error): {e}", exc_info=True)
            return 0  # Fail-safe: return 0 on error
        except Exception as e:
            # Unexpected errors
            logger.error(f"[FileStorage] Error getting daily free usage for user {user_id}, model {model_id} (unexpected error): {e}", exc_info=True)
            return 0  # Fail-safe: return 0 on error
    
    async def get_hourly_free_usage(self, user_id: int, model_id: str) -> int:
        """
        Get hourly free usage count for a user and model in FileStorage.
        
        Args:
            user_id: User ID
            model_id: Model ID
            
        Returns:
            Count of free usages in current hour
        """
        # CRITICAL: Validate inputs
        if not isinstance(user_id, int) or user_id <= 0:
            logger.warning(f"[FileStorage] Invalid user_id in get_hourly_free_usage: {user_id}")
            return 0
        
        if not isinstance(model_id, str) or not model_id.strip():
            logger.warning(f"[FileStorage] Invalid model_id in get_hourly_free_usage: {model_id}")
            return 0
        
        try:
            now = datetime.now()
            hour_start = now.replace(minute=0, second=0, microsecond=0)
            
            async with self.lock:
                # Use in-memory data (already synced from JSON)
                count = 0
                for entry in self._free_usage:
                    if not isinstance(entry, dict):
                        logger.warning(f"[FileStorage] Invalid entry type in _free_usage: {type(entry)}")
                        continue
                    
                    if entry.get("user_id") == user_id and entry.get("model_id") == model_id:
                        try:
                            created_at_str = entry.get("created_at")
                            if not created_at_str:
                                logger.warning(f"[FileStorage] Missing created_at in free_usage entry: {entry}")
                                continue
                            
                            created_at = datetime.fromisoformat(created_at_str)
                            if created_at >= hour_start:
                                count += 1
                        except (ValueError, KeyError, TypeError) as e:
                            logger.warning(f"[FileStorage] Invalid created_at format in free_usage: {entry.get('created_at')}, error: {e}")
                return count
        except (KeyError, TypeError, ValueError) as e:
            # Specific errors (missing keys, type errors, value errors)
            logger.error(f"[FileStorage] Error getting hourly free usage for user {user_id}, model {model_id} (data error): {e}", exc_info=True)
            return 0  # Fail-safe: return 0 on error
        except Exception as e:
            # Unexpected errors
            logger.error(f"[FileStorage] Error getting hourly free usage for user {user_id}, model {model_id} (unexpected error): {e}", exc_info=True)
            return 0  # Fail-safe: return 0 on error
    
    async def delete_free_usage(self, user_id: int, model_id: str, job_id: str) -> None:
        """
        Delete a specific free usage record. Used when a free generation fails.
        BATCH 48.44: Allow deleting failed free usage.
        """
        if not isinstance(user_id, int) or user_id <= 0:
            raise ValueError(f"Invalid user_id: {user_id} (must be positive integer)")
        if not isinstance(model_id, str) or not model_id.strip():
            raise ValueError(f"Invalid model_id: {model_id} (must be non-empty string)")
        if not isinstance(job_id, str) or not job_id.strip():
            raise ValueError(f"Invalid job_id: {job_id} (must be non-empty string)")
        
        async with self.lock:
            # Remove from memory
            original_count = len(self._free_usage)
            self._free_usage = [
                entry for entry in self._free_usage
                if not (entry.get("user_id") == user_id and 
                       entry.get("model_id") == model_id and 
                       entry.get("job_id") == job_id)
            ]
            removed_count = original_count - len(self._free_usage)
            
            if removed_count > 0:
                # BATCH 48.44: Save to JSON file
                data = await self._load_data()
                data["free_usage"] = self._free_usage
                await self._save_data(data)
                logger.info(f"[FileStorage] Deleted {removed_count} free usage record(s): user={user_id}, model={model_id}, job={job_id}")
            else:
                logger.debug(f"[FileStorage] No free usage record found to delete: user={user_id}, model={model_id}, job={job_id}")

    async def ensure_user(
        self,
        user_id: int,
        username: Optional[str] = None,
        first_name: Optional[str] = None,
        last_name: Optional[str] = None
    ) -> None:
        """
        Ensure user exists in storage (create if not exists, update if changed).
        CRITICAL: Call this BEFORE creating jobs to avoid FK violations.
        
        Args:
            user_id: Telegram user ID
            username: Telegram username (optional)
            first_name: User first name (optional)
            last_name: User last name (optional)
        """
        # CRITICAL: Validate input
        if not isinstance(user_id, int) or user_id <= 0:
            raise ValueError(f"Invalid user_id: {user_id} (must be positive integer)")
        
        # CRITICAL: Validate optional fields
        if username is not None and not isinstance(username, str):
            logger.warning(f"[FileStorage] Invalid username type in ensure_user: {type(username)}")
            username = str(username) if username else None
        
        if first_name is not None and not isinstance(first_name, str):
            logger.warning(f"[FileStorage] Invalid first_name type in ensure_user: {type(first_name)}")
            first_name = str(first_name) if first_name else None
        
        if last_name is not None and not isinstance(last_name, str):
            logger.warning(f"[FileStorage] Invalid last_name type in ensure_user: {type(last_name)}")
            last_name = str(last_name) if last_name else None
        
        try:
            async with self.lock:
                data = await self._load_data()
                
                # CRITICAL: Validate data structure
                if not isinstance(data, dict):
                    logger.error(f"[FileStorage] Invalid data structure in ensure_user: {type(data)}")
                    raise ValueError("Invalid data structure")
                
                if "users" not in data:
                    data["users"] = {}
                
                user_key = str(user_id)
                
                # Create user entry if not exists
                if user_key not in data["users"]:
                    data["users"][user_key] = {
                        "balance": 0.0,
                        "created_at": datetime.now().isoformat()
                    }
                
                # Update user info if provided
                user_data = data["users"][user_key]
                if username is not None:
                    user_data["username"] = username
                if first_name is not None:
                    user_data["first_name"] = first_name
                if last_name is not None:
                    user_data["last_name"] = last_name
                
                user_data["updated_at"] = datetime.now().isoformat()
                
                # Save to file
                await self._save_data(data)
                
                logger.debug(f"[FileStorage] âœ… User ensured: user_id={user_id}, username={username}")
        except (ValueError, TypeError, KeyError) as e:
            # Specific errors (validation, type errors, missing keys)
            logger.error(f"[FileStorage] Error ensuring user {user_id} (validation error): {e}", exc_info=True)
            raise
        except Exception as e:
            # Unexpected errors
            logger.error(f"[FileStorage] Error ensuring user {user_id} (unexpected error): {e}", exc_info=True)
            raise
    
    async def is_update_processed(self, update_id: int) -> bool:
        """
        Check if update_id has been processed (dedup check).
        
        BATCH 48.26: FileStorage implementation - uses in-memory set + file persistence.
        
        Returns:
            True if update was already processed, False otherwise
        """
        # BATCH 48.26: Validate input
        if not isinstance(update_id, int) or update_id <= 0:
            return False
        
        async with self.lock:
            data = await self._load_data()
            
            # Get processed_updates from metadata
            processed_updates = data.get("metadata", {}).get("processed_updates", {})
            
            # Check if update_id exists
            return str(update_id) in processed_updates
    
    async def mark_update_processed(
        self, 
        update_id: int, 
        worker_id: str = "unknown", 
        update_type: str = "unknown"
    ) -> bool:
        """
        Mark update_id as processed (dedup insert).
        
        BATCH 48.26: FileStorage implementation - stores in metadata.processed_updates.
        
        CRITICAL: Uses file lock to prevent race condition when two workers
        simultaneously try to process the same update_id.
        
        Returns:
            True if successfully marked (this worker won the race), False if already existed
        """
        # BATCH 48.26: Validate input
        if not isinstance(update_id, int) or update_id <= 0:
            return False
        
        async with self.lock:
            data = await self._load_data()
            
            # Initialize processed_updates if not exists
            if "metadata" not in data:
                data["metadata"] = {}
            if "processed_updates" not in data["metadata"]:
                data["metadata"]["processed_updates"] = {}
            
            processed_updates = data["metadata"]["processed_updates"]
            update_key = str(update_id)
            
            # Check if already processed
            if update_key in processed_updates:
                return False
            
            # Mark as processed
            processed_updates[update_key] = {
                "processed_at": datetime.now().isoformat(),
                "worker_id": worker_id,
                "update_type": update_type
            }
            
            # Cleanup old entries (keep last 10000 updates)
            if len(processed_updates) > 10000:
                # Sort by processed_at and keep newest
                sorted_updates = sorted(
                    processed_updates.items(),
                    key=lambda x: x[1].get("processed_at", ""),
                    reverse=True
                )
                processed_updates = dict(sorted_updates[:10000])
                data["metadata"]["processed_updates"] = processed_updates
            
            # Save to file
            await self._save_data(data)
            
            logger.debug(f"âœ… Update marked as processed: update_id={update_id}, worker_id={worker_id}")
            return True
    
    async def pull_from_github(self):
        """
        Pull latest data from GitHub.
        Should be called on bot startup.
        
        BATCH 48.11: Use git_integration (centralized, DRY).
        """
        try:
            from app.utils.git_integration import git_pull
            
            logger.info("ðŸ“¥ Pulling latest balances from GitHub...")
            success = await git_pull()
            
            if success:
                logger.info("âœ… Pulled from GitHub: up to date")
            else:
                logger.warning("âš ï¸ Git pull failed (will continue with local data)")
        
        except Exception as e:
            logger.error(f"âŒ Git pull error: {e}")
    
    async def _commit_to_github(self, commit_message: str, user_id: int = 0, old_balance: float = 0.0, new_balance: float = 0.0):
        """
        Commit changes to GitHub with GUARANTEED push.
        
        COMPREHENSIVE AUDIT FIX: Uses BalanceGuaranteeSystem (async, non-blocking).
        NO sync fallback - BalanceGuaranteeSystem handles retries!
        """
        try:
            # BATCH 48.8: Use BalanceGuaranteeSystem for guaranteed push
            from app.storage.balance_guarantee import get_balance_guarantee_system
            
            guarantee_system = get_balance_guarantee_system()
            success = await guarantee_system.guaranteed_git_push(
                file_path=self.data_file,
                commit_message=commit_message,
                user_id=user_id,
                old_balance=old_balance,
                new_balance=new_balance,
                reason="balance_update"
            )
            
            if not success:
                logger.error(f"âŒ CRITICAL: Failed to queue git push (will retry): {commit_message}")
        except Exception as e:
            logger.error(f"âŒ Auto-commit failed: {e}", exc_info=True)
    
    # COMPREHENSIVE AUDIT: Removed _git_commit_sync (dead code, replaced by BalanceGuaranteeSystem)


# Global instance
_file_storage: Optional[FileStorage] = None


def get_file_storage() -> FileStorage:
    """Get global FileStorage instance."""
    global _file_storage
    if _file_storage is None:
        _file_storage = FileStorage()
    return _file_storage


async def init_file_storage():
    """
    Initialize file storage and pull latest data from GitHub.
    
    BATCH 48.9: Smart file discovery + deploy awareness.
    Should be called on bot startup.
    """
    # BATCH 48.4: Configure git for Render (if not configured)
    try:
        from app.utils.git_integration import configure_git_for_render
        configure_git_for_render()
    except Exception as e:
        logger.warning(f"[BATCH48] Failed to configure git: {e}")
    
    # BATCH 48.9: Pull FIRST (get latest balances from GitHub)
    storage = get_file_storage()
    logger.info("ðŸ“¥ Pulling latest balances from GitHub (BEFORE file discovery)...")
    await storage.pull_from_github()
    
    # BATCH 48.9: Smart file discovery
    try:
        from app.storage.file_discovery import (
            discover_balance_file,
            check_multi_bot_conflicts,
            log_balance_file_status,
            ensure_balance_file_in_git,
            verify_balance_file_integrity
        )
        
        # Discover balance file for this bot
        balance_file = discover_balance_file()
        logger.info(f"ðŸ” Discovered balance file: {balance_file}")
        
        # Check for multi-bot conflicts
        conflicts = check_multi_bot_conflicts()
        if conflicts.get("other_bots"):
            logger.info(f"ðŸ¤– Other bots detected: {len(conflicts['other_bots'])}")
            for other_bot in conflicts["other_bots"]:
                logger.info(f"  - Bot {other_bot['bot_id']}: {other_bot['file']}")
        
        # Log balance file status
        log_balance_file_status(balance_file)
        
        # Ensure file is tracked in git
        ensure_balance_file_in_git(balance_file)
        
        # Verify file integrity
        if balance_file.exists():
            is_valid = verify_balance_file_integrity(balance_file)
            if not is_valid:
                logger.error("âŒ CRITICAL: Balance file integrity check FAILED!")
                # Try to restore from backup
                backup_file = balance_file.with_suffix('.json.backup')
                if backup_file.exists():
                    logger.warning("ðŸ”„ Restoring from backup...")
                    import shutil
                    shutil.copy2(backup_file, balance_file)
                    logger.info("âœ… Restored from backup")
                    # Verify again
                    is_valid = verify_balance_file_integrity(balance_file)
                    if is_valid:
                        logger.info("âœ… Backup file is valid")
                    else:
                        logger.error("âŒ CRITICAL: Backup file is also invalid!")
    
    except Exception as e:
        logger.error(f"âŒ File discovery failed: {e}")
    
    logger.info("âœ… FileStorage ready (NO DATABASE MODE)")


# ============================================================================
# COMPATIBILITY LAYER - Ð´Ð»Ñ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸ ÑÐ¾ ÑÑ‚Ð°Ñ€Ñ‹Ð¼ ÐºÐ¾Ð´Ð¾Ð¼
# ============================================================================

async def get_user_balance_async(user_id: int) -> float:
    """Compatibility wrapper for old code."""
    storage = get_file_storage()
    return await storage.get_balance(user_id)


async def add_user_balance(user_id: int, amount: float):
    """Compatibility wrapper for old code."""
    storage = get_file_storage()
    await storage.add_balance(user_id, amount)


async def subtract_user_balance(user_id: int, amount: float) -> bool:
    """Compatibility wrapper for old code."""
    storage = get_file_storage()
    return await storage.subtract_balance(user_id, amount)

